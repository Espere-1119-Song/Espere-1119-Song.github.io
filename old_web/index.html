---
layout: page
title: Enxin(Espere) Song
subtitle: master @ZJU 
use-site-title: false
---

<head>
	<style>
		a { text-decoration : none; }
		a:hover { text-decoration : underline; }
		a, a:visited { color : #b5194f; }
	</style>
	<script src="https://kit.fontawesome.com/5bef57b3e9.js" crossorigin="anonymous"></script>
</head>

<br>
Enxin Song is a master student at Zhejiang University, with <a href="https://cvnext.github.io/">CVNext Lab</a> advised by Prof. <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. She was fortunate to have internship at Media Computing Group, Microsoft Research Asia, advised by Dr. <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>.Her research interests include image generation, video understanding, and multi-modality learning.
<br>

<br>
<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-regular fa-note-sticky" style="font-size:24px"></i> Selected Publications:</b>
<p><font color="grey" size="3">
Also see <a href="https://scholar.google.com/citations?user=sLqa-3oAAAAJ&hl=en" target="_blank">Google Scholar</a>.
</font></p>

<ul>
	<li>
		<p style="font-size:16px"> 
			<strong>
			MovieChat: From Dense Token to Sparse Memory for Long Video Understanding
			</strong>
			<br>
			<b>Enxin Song*</b>, Wenhao Chai*, Guanhong Wang*, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, Gaoang Wang✉
			<br>
			<br>
			<a href="https://rese1f.github.io/MovieChat/">[Website]</a>
			<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html">[Paper]</a>
			<a href="https://github.com/rese1f/MovieChat">[Code]</a>
			<a href="https://huggingface.co/datasets/Enxin/MovieChat-1K_train">[Benchmark]</a>
			<img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
			<br>
			<font color="grey" size="2">
			 MovieChat achieves state-of-the-art performace in extra long video (more than 10K frames) understanding by introducing memory mechanism. 
			</font>
	  	</p>
	</li>

	<li>
		<p style="font-size:16px"> 
			<strong>
			MovieChat+: Question-aware Sparse Memory for Long Video Question Answering
			</strong>
			<br>
			<b>Enxin Song*</b>, Wenhao Chai*, Tian Ye, Jenq-Neng Hwang, Xi Li, Gaoang Wang✉
			<br>
			<br>
			<a href="https://arxiv.org/abs/2404.17176">[Paper]</a>
			<a href="https://github.com/rese1f/MovieChat">[Code]</a>
			<img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
			<br>
			<font color="grey" size="2">
			 A novel framework that integrating question-aware sparse memory to conduct long video understanding tasks. 
			</font>
	  	</p>
	</li>

	<li>
		<p style="font-size:16px"> 
			<strong>
			AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark
			</strong>
			<br>
			Wenhao Chai*†, <b>Enxin Song*</b>,, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, Christopher D. Manning
			<br>
			<br>
			<a href="https://rese1f.github.io/aurora-web/">[Website]</a>
			<a href="https://arxiv.org/abs/2410.03051">[Paper]</a>
			<a href="https://huggingface.co/collections/wchai/auroracap-66d117ffe13bedda96702013">[Model]</a>
			<a href="https://huggingface.co/datasets/wchai/Video-Detailed-Caption">[Benchmark]</a>
			<a href="https://huggingface.co/datasets/wchai/AuroraCap-trainset">[Dataset]</a>
			<a href="https://github.com/rese1f/aurora">[Code]</a>
			<img alt="NPM" src="https://img.shields.io/github/stars/rese1f/aurora?style=social">
			<br>
			<font color="grey" size="2">
			 AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning.
			</font>
	  	</p>
	</li>
	
</ul>


<br>
<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>
<ul>
	<li><i>Oct. 2024:</i> We submit some baseline (AuroraCap, MovieChat, MovieChat-Onevision) and some benchmark (VDC, MovieChat-1K) to <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a> (LLaVA team), which enables quick evaluation with just a single line of code.
	</li><br>
	
	<li><i>Oct. 2024:</i> We release a new version of MovieChat, which use LLaVA-OneVision as the base model instead of the original VideoLLaMA. The new version is available on <a href="https://github.com/rese1f/MovieChat/tree/main/MovieChat_Onevision">Github</a>.
	</li><br>

	<li><i>Oct. 2024:</i>  Our paper <i>Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</i> is released, and the code is available at <a href="https://huggingface.co/MeissonFlow/Meissonic">website</a>.
	</li><br>

	<li><i>Oct. 2024:</i>  Our paper <i>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</i> is released, and the code is available at <a href="https://github.com/rese1f/aurora">website</a>.
	</li><br>
	
	<li><i>Jun. 2024:</i>  We finish CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/home">LOVEU</a> and presented a summary of the competition results offline.</a>
	</li><br>
	
	<li><i>May 2024:</i>  Finish my research internship at Microsoft Research Asia (MSRA), Beijing.</a>
	</li><br>
	
	<li><i>Apr. 2024:</i> <i class="fa-solid fa-trophy"></i> We are hosting CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/track1">LOVEU</a>
	</li><br>

	<li><i>Apr. 2024:</i> Our paper <i>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</i> is released, and the code is available at <a href="https://rese1f.github.io/MovieChat/">website</a>.
	</li><br>
	
	<li><i>Feb. 2024:</i> Our paper <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is accepted by Computer Vision and Pattern Recognition (CVPR), 2024.
	</li><br>
	
	<li><i>Nov. 2023:</i> <img src="static/imgs/microsoft.png" width="15" height="15" style="vertical-align:text-bottom"/> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>.
	</li><br>

	<li><i>Sep. 2023:</i>  Our paper <i>Devil in the Number: Towards Robust Multi-modality Data Filter</i> is accepted by ICCV 2023 workshop: <a href="https://www.datacomp.ai/">TNGCV-DataComp</a>.
	</li><br>
	
	<li><i>Jul. 2023:</i>  Our project <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is released at <a href="https://rese1f.github.io/MovieChat/">website</a>.
	</li><br>
	
	<li><i>Jun. 2023:</i> <img src="static/imgs/dlut.png" width="18" height="18" style="vertical-align:text-bottom"/> I graduate from Dalian University of Technology.
	</li><br>

	<li><i>Oct. 2022:</i> Start my research on domain adaptation task for image caption, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>.
	</li><br>

</ul>

<!DOCTYPE html>
<html>

  <head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-RFR2GFKT75"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-RFR2GFKT75');
  </script> -->
  <!-- Google Tag Manager -->
  <!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  })(window,document,'script','dataLayer','GTM-NC38VTD');</script> -->
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


  <meta name="viewport" content="width=device-width, initial-scale=1">
	  
	<!-- <meta name="google-site-verification" content="u_--0s_Uw50r4zCgRrzltkmFhNCduYOJwPGlKVTuNJU" /> -->

  <title>Enxin Song 宋恩欣</title>
  <meta name="description" lang="en" content="This is an academic website for Enxin Song">
  <meta name="keywords" lang="en" content="Enxin Song." />
  

  <link rel="shortcut icon" href="old_web/static/imgs/favicon.ico">
  <link rel="stylesheet" href="main.css?v=20240510">
  <link rel="stylesheet" href="css/custom.css?v=20240510">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="stylesheet" href="css/academicons.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">



  <script src="js/jquery-2.1.3.min.js"> </script>

  <!--[if lt IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/r29/html5.min.js">
</script>
<![endif]-->

  <script type="text/javascript">
    function ShowHide(divId, linkDivId) {
        if(document.getElementById(divId).style.display == 'none') {
            document.getElementById(divId).style.display='block';
            document.getElementById(linkDivId).style.display='none'; // Hide the "read more" link
        }
        else {
            document.getElementById(divId).style.display = 'none';
        }
    }
  </script>

  <style>
   
    li.indented {
        position: relative;
        padding-left: 90px; 
    }

    li.indented::before {
        content: attr(data-date);
        position: absolute;
        left: 0;
        top: 0;
        white-space: nowrap;
    }
  </style>
  
</head>


<body>

    <!-- Google Tag Manager (noscript) -->
<!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NC38VTD"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> -->
<!-- End Google Tag Manager (noscript) -->

    <header class="site-header">

  <div class="wrapper">
<nav class="site-nav">
  <a href="#" class="menu-icon">
    <svg viewBox="0 0 18 15">
      <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
      <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
      <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
    </svg>
  </a>

  <div class="trigger">
    <a class="page-link" href="#bio"><b>Biography</b></a>
    <a class="page-link" href="#timeline"><b>Education</b></a>
    <a class="page-link" href="#timeline"><b>Experiences</b></a>
    <a class="page-link" href="#publications"><b>Publications</b></a>
    <a class="page-link" href="#awards"><b>Honors & Awards</b></a>
    <a class="page-link" href="#teaching"><b>Teaching </b></a>
    <a class="page-link" href="#academicservices"><b>Professional Services </b></a>
    <!-- <a class="page-link" href="#misc"><b>CV</b></a> -->
    <!--<a class="page-link" href="#otherprojects">Projects</a>-->
    <!-- Button for Posts -->
  </div>
</nav>
  </div>
</header>

  <div id="profile-cover" class="cover shallow-bg img-responsive"> 
    <div id="profile-namecard" class="profile-wrapper wrapper-light">
      <div id="my-pic" class="profile-col profile-col-1">
        <img id="profile-avatar" src="./images/self-portrait/me.JPEG" alt="Me" class="circle-img border-dark"-/>
      </div>
      <div id="my-contact" class="profile-col profile-col-2">
        <div id="my-name" class="text-grey-dark">
          Enxin Song (宋恩欣)<br>
        </div>
        <div id="my-title" class="text-grey">
          <!-- Math Undergrad @ ZJU 24'<br> -->
          Master Student @ ZJU
        </div>
        <!-- <div id="my-email" class="text-grey-light">
          wjzhao1203[at]gmail.com
        </div> -->
        <div class="social-media">
          <a href="https://scholar.google.com.hk/citations?user=sLqa-3oAAAAJ&hl=zh-CN" class="icon-button github">
          <i class="ai ai-google-scholar-square icon-github"></i>
            <span></span>
          </a>

          <a href="https://github.com/Espere-1119-Song" class="icon-button github">
            <i class="fa fa-github icon-github"></i>
            <span></span>
          </a>

          <a href="https://linkedin.com/in/enxinsong-57b471286/" class="icon-button linkedin">
            <i class="fa fa-linkedin icon-linkedin"></i>
            <span></span>
          </a>
          
          <a href="https://twitter.com/EnxinSong" class="icon-button twitter">
            <i class="fa fa-twitter icon-twitter"></i>
            <span></span>
          </a>
         
          <a href="mailto:enxin.23@intl.zju.edu.cn" class="icon-button github">
            <i class="fa fa-envelope icon-github"></i>
            <span></span>
          </a>
        </div>
      </div>

      <!-- <div id="my-desc" class="profile-col profile-col-2 hide">
        <div id="my-desc-title">
          Another me.
        </div>
        <div id="my-desc-content">
          I enjoy running, traveling, hiking, movies, music and so much more<br>
        </div>
      </div> -->
    </div>
  </div>
  <!-- <script type="text/javascript">
    function deepFeature()
    {
      $('#profile-avatar').attr('src', 'images/self-portrait/deep-me.jpg');
      $('#profile-avatar').removeClass('border-dark');
      $('#profile-avatar').addClass('border-bright');

      $('#profile-cover').removeClass('shallow-bg');
      $('#profile-cover').addClass('deep-bg');
      
      $('#profile-namecard').removeClass('wrapper-light');
      $('#profile-namecard').addClass('wrapper-dark');
      
      $('#my-desc').removeClass('hide');
      $('#my-contact').addClass('hide');    
      // console.log('Move in now...');
    }

    function shallowFeature()
    {
      $('#profile-avatar').attr('src', 'images/self-portrait/me.jpg');
      $('#profile-avatar').removeClass('border-bright');
      $('#profile-avatar').addClass('border-dark');

      $('#profile-cover').removeClass('deep-bg');
      $('#profile-cover').addClass('shallow-bg');
      
      $('#profile-namecard').removeClass('wrapper-dark');
      $('#profile-namecard').addClass('wrapper-light');
      
      $('#my-contact').removeClass('hide');
      $('#my-desc').addClass('hide');
      // console.log('Move out now...');
    }

    $(function() { 
      $('#my-pic').hover(deepFeature, shallowFeature);
    })

  </script> -->

<div class="page-content">
<div class="wrapper">
<div id="bio" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-id-card" aria-hidden="true"></i>
    About
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <p> 
      Enxin Song is a research intern at the University of California, San Diego (<a href="https://www.ucsd.edu/">UCSD</a>) under Prof. <a href="https://pages.ucsd.edu/~ztu/">Zhuowen Tu</a>. 
      She will receive her M.S. in March 2026 from <a href="https://zjui.intl.zju.edu.cn//">Zhejiang University</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=GhsXNiwAAAAJ&hl=zh-CN">Gaoang Wang</a> (<a href="https://cvnext.github.io/">CVNext Lab</a>), and holds a B.S. in Software Engineering from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>. 
      Previously, she was a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research Asia</a>.
      Enxin stays curious about the wider landscape of computer vision and deep learning, and actively seeks new collaboration opportunities.
      Her work centers on video understanding, highlighted by MovieChat, the first Large Mutli-Modal Model for hour-long video understanding.
      She has co-organized workshops and challenges on video understanding at CVPR 2024 and 2025.
      She is a highly self-motivated, and curious student applying to <strong>Ph.D. programs for 2026Fall</strong>.
      You can view her <a href="Enxin Song-CV.pdf" target="_blank" rel="noopener">Curriculum Vitae</a> and <a href="20250501154022170.pdf" target="_blank" rel="noopener">undergraduate transcript</a>.
      <br>
    </p>
  </div>
</div>


<div id="Experiences" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    Experiences
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <div class="timeline">
      <article class="timeline-entry">
        <div class="timeline-entry__meta">
          <span class="timeline-entry__time">Feb. 2025 – Present</span>
          <img src="images/icon/UCSD.svg" alt="UCSD" class="timeline-entry__logo" width="50" height="50">
        </div>
        <div class="timeline-entry__content">
          <h3>University of California, San Diego (UCSD), USA</h3>
          <p class="timeline-entry__role">Visiting Intern</p>
          <p>Advised by <a href="https://pages.ucsd.edu/~ztu/">Prof. Zhuowen Tu</a>.</p>
        </div>
      </article>

      <article class="timeline-entry">
        <div class="timeline-entry__meta">
          <span class="timeline-entry__time">Nov. 2023 – May 2024</span>
          <img src="images/icon/msra.jpg" alt="Microsoft Research Asia" class="timeline-entry__logo" width="48" height="48">
        </div>
        <div class="timeline-entry__content">
          <h3>Media Computing Group, Microsoft Research Lab - Asia, Beijing</h3>
          <p class="timeline-entry__role">Research Intern</p>
          <p>Worked on text-to-image generation.</p>
        </div>
      </article>
    </div>
  </div>
  <br>
</div>


<div id="Education" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-graduation-cap" aria-hidden="true"></i>
    Education
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <div class="timeline">
      <article class="timeline-entry">
        <div class="timeline-entry__meta">
          <span class="timeline-entry__time">Sep. 2023 – Mar. 2026 (expected)</span>
          <img src="images/icon/zju.png" alt="Zhejiang University" class="timeline-entry__logo" width="48" height="48">
        </div>
        <div class="timeline-entry__content">
          <h3>M.S., Zhejiang University, Hangzhou, China</h3>
          <p class="timeline-entry__role">Artificial Intelligence</p>
          <p>Ranked 1/82 in the M.S. program.</p>
        </div>
      </article>

      <article class="timeline-entry">
        <div class="timeline-entry__meta">
          <span class="timeline-entry__time">Sep. 2019 – Jun. 2023</span>
          <img src="images/icon/dlut.png" alt="Dalian University of Technology" class="timeline-entry__logo" width="48" height="48">
        </div>
        <div class="timeline-entry__content">
          <h3>B.S., Dalian University of Technology (DLUT), Dalian, China</h3>
          <p class="timeline-entry__role">Software Engineering</p>
          <p>Ranked 21/385 in the undergraduate cohort.</p>
        </div>
      </article>
    </div>
  </div>
  <br>
</div>





<div id="news" class="bio">
  <h1 class="md-heading text-left">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    News
  </h1>  
  <ul class="news-list">
    <li class="news-list__item">
      <span class="news-list__date">Nov 2025</span>
      <span class="news-list__text">Selected (top 10%) to give a talk at the KAUST Rising Stars in AI Symposium 2026.</span>
    </li>
    <li class="news-list__item">
      <span class="news-list__date">Oct 2025</span>
      <span class="news-list__text">Video-MMLU received the <a href="https://knowledgemr-workshop.github.io/#accepted-paper">Outstanding Paper Award</a> at the ICCV 2025 Knowledge-Intensive Multimodal Reasoning Workshop, along with a travel grant.</span>
    </li>
    <li class="news-list__item">
      <span class="news-list__date">Oct 2025</span>
      <span class="news-list__text">We release <a href="https://enxinsong.com/VideoNSA-web/">VideoNSA</a>, a hardware-aware native sparse attention mechanism for video understanding.</span>
    </li>
    <li class="news-list__item">
      <span class="news-list__date">Sep 2025</span>
      <span class="news-list__text">Invited talk at <a href="https://lambda.ai/">Lambda AI</a> titled <i>From Seeing to Thinking</i>.</span>
    </li>
    <li class="news-list__item">
      <span class="news-list__date">Sep 2025</span>
      <span class="news-list__text">One paper accepted by ICCV 2025 KnowledgeMR Workshop.</span>
    </li>
    <li class="news-list__item">
      <span class="news-list__date">Aug 2025</span>
      <span class="news-list__text">Our paper <i>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</i> is accepted by IEEE TPAMI.</span>
    </li>
    <!-- <li class="news-list__item">
      <span class="news-list__date">Jul 2025</span>
      <span class="news-list__text">Our paper <i>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</i> is accepted by ICCV 2025 Findings.</span>
    </li> -->
  </ul>
  <!-- Additional historical updates retained below for reference
    <li class="indented" data-date="[Jun, 2025]"> Our paper <i>Bringing RNNs Back to Efficient Open-Ended Video Understanding</i> is accepted by ICCV 2025. </li>
    <li class="indented" data-date="[Apr, 2025]"> We host the CVPR 2025 Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr25/track1a">LOVE Track 1A</a> and <a href="https://sites.google.com/view/loveucvpr25/track1b">Track 1B</a>. </li>
    <li class="indented" data-date="[Apr, 2025]"> We release <a href="https://arxiv.org/pdf/2504.14693">Video-MMLU</a>, a Massive Multi-Discipline Lecture Understanding Benchmark. </li>
    <li class="indented" data-date="[Mar, 2025]"> One paper accepted to the CVPR 2025 Efficient Large Vision Models Workshop. </li>
    <li class="indented" data-date="[Jan, 2025]"> Two papers are accepted by ICLR 2025. </li>
    <li class="indented" data-date="[Jan, 2025]"> Our paper <i>Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis</i> is accepted by ICLR 2025. </li>
    <li class="indented" data-date="[Oct, 2024]"> We submit some baseline (AuroraCap, MovieChat, MovieChat-Onevision) and some benchmark (VDC, MovieChat-1K) to <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a> (LLaVA team), which enables quick evaluation with just a single line of code. </li>
    <li class="indented" data-date="[Oct, 2024]"> We release a new version of MovieChat, which use LLaVA-OneVision as the base model instead of the original VideoLLaMA. The new version is available on <a href="https://github.com/rese1f/MovieChat/tree/main/MovieChat_Onevision">Github</a>. </li>
    <li class="indented" data-date="[Oct, 2024]"> Our paper <i>Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</i> is released, and the code is available at <a href="https://huggingface.co/MeissonFlow/Meissonic">website</a>. </li>
    <li class="indented" data-date="[Oct, 2024]"> Our paper <i>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</i> is released, and the code is available at <a href="https://github.com/rese1f/aurora">website</a>. </li>
    <li class="indented" data-date="[Jun, 2024]"> We finish CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/home">LOVEU</a> and presented a summary of the competition results offline.</a> </li>
    <li class="indented" data-date="[May, 2024]"> Finish my research internship at Microsoft Research Asia (MSRA), Beijing.</a> </li>
    <li class="indented" data-date="[Apr, 2024]"> We are hosting CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/track1">LOVEU</a> </li>
    <li class="indented" data-date="[Apr, 2024]"> Our paper <i>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</i> is released, and the code is available at <a href="https://rese1f.github.io/MovieChat/">website</a>. </li>
    <li class="indented" data-date="[Feb, 2024]"> Our paper <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is accepted by Computer Vision and Pattern Recognition (CVPR), 2024. </li>
    <li class="indented" data-date="[Nov, 2023]"> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>. </li>
    <li class="indented" data-date="[Sep, 2023]"> Our paper <i>Devil in the Number: Towards Robust Multi-modality Data Filter</i> is accepted by ICCV 2023 workshop: <a href="https://www.datacomp.ai/">TNGCV-DataComp</a>. </li>
    <li class="indented" data-date="[Jul, 2023]"> Our project <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is released at <a href="https://rese1f.github.io/MovieChat/">website</a>. </li>
    <li class="indented" data-date="[Jun, 2023]"> I graduate from Dalian University of Technology. </li>
    <li class="indented" data-date="[Oct, 2022]"> Start my research on domain adaptation task for image caption, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. </li>
  -->
</div>

<br>

<div id="publications" class="publications">
  <h1 class="md-heading text-left">
    <i class="fa fa-file" aria-hidden="true"></i>
    Selected Publications and Manuscripts
  </h1>
  <p>
    * Equal contribution.
  </p>
  <p>
    Also see <a href="https://scholar.google.com/citations?user=sLqa-3oAAAAJ&hl=en" target="_blank">Google Scholar</a>.
  </p>

  <div class="pub-list">
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/VideoNSA.png">
      </div>
      <div class="pub-right">
        <div class="title">
          VideoNSA: Native Sparse Attention Scales Video Understanding
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai</a>,
            <a class="author" href="https://scholar.google.com/citations?user=v6dmW5cntoMC">Shusheng Yang</a>,
            <a class="author" href="https://ethana.org/">Ethan J. Armand</a>,
            <a class="author" href="https://shanxiaojun.github.io/">Xiaojun Shan</a>,
            <a class="author" href="https://scholar.google.com/citations?user=3CK77ZcAAAAJ&hl=en">Haiyang Xu</a>,
            <a class="author" href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>,
            <a class="author" href="https://pages.ucsd.edu/~ztu/">Zhuowen Tu</a>
          </font>
          </span>
        </div>
        <div class="publish">
          <span class="publisher">Preprint, 2025</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://arxiv.org/abs/2510.02295">Paper</a>
          <a class="tag" href="https://enxinsong.com/VideoNSA-web/">Website</a>
          <a class="tag" href="https://huggingface.co/Enxin/VideoNSA">Model</a>
          <a class="tag" href="https://github.com/Espere-1119-Song/VideoNSA">Code</a>
          <img alt="GitHub stars" src="https://img.shields.io/github/stars/Espere-1119-Song/VideoNSA?style=social">
        </div>
        <span >VideoNSA delivers hardware-aware native sparse attention primitives for efficient video understanding systems.</span>
      </div>
    </div>

    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/videommlu.png">
      </div>
      <div class="pub-right">
        <div class="title">
          Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai</a>,
            <a class="author" href="https://weili-0234.github.io/">Weili Xu</a>,
            <a class="author" href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>,
            <a class="author" href="">Yuxuan Liu</a>,
            <a class="author" href="https://omerbt.github.io/">Gaoang Wang</a>,
          </font>
          </span>
        </div>
        <div class="publish">
          <span class="publisher">ICCVW, 2025</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://arxiv.org/abs/2504.14693">Paper</a>
          <a class="tag" href="https://enxinsong.com/Video-MMLU-web/">Website</a>
          <a class="tag" href="https://huggingface.co/datasets/Enxin/Video-MMLU">Benchmark</a>
          <a class="tag" href="https://github.com/Espere-1119-Song/Video-MMLU">Code</a>
          <img alt="NPM" src="https://img.shields.io/github/stars/Espere-1119-Song/Video-MMLU?style=social">
        </div>
        <span >Video-MMLU is a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. </span>
      </div>
    </div>

	<div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/auroralong.png">
      </div>
      <div class="pub-right">
        <div class="title">
          AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
			<a class="author" href="https://weili-0234.github.io/">Weili Xu*</a>,
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai</a>,
			<a class="author" href="">Xuexiang Wen</a>,
			<a class="author" href="https://owen718.github.io/">Tian Ye</a>,
            <a class="author" href="https://cvnext.github.io/">Gaoang Wang</a>,
          </font>
          </span>
        </div>
        <div class="publish">
          <span class="publisher">ICCV, 2025</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://arxiv.org/abs/2507.02591">Paper</a>
        </div>
        <span >Video-MMLU uses a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states to solve long video understanding tasks. </span>
      </div>
    </div>
    
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/auroracap.JPG">
      </div>
      <div class="pub-right">
        <div class="title">
          AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*</a>,
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://yilundu.github.io/">Yilun Du</a>,
            <a class="author" href="https://cs.stanford.edu/~chenlin/">Chenlin Meng</a>,
            <a class="author" href="https://scholar.google.com/citations?user=WjF1dugAAAAJ">Vashisht Madhavan</a>,
            <a class="author" href="https://omerbt.github.io/">Omer Bar-Tal</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jeng-Neng Hwang</a>,
            <a class="author" href="https://www.sainingxie.com/">Saining Xie</a>,
            <a class="author" href="https://nlp.stanford.edu/~manning/">Christopher D. Manning</a>
          </font>
          </span>
        </div>
        <div class="publish">
          <span class="publisher">ICLR, 2025</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://arxiv.org/abs/2410.03051">Paper</a>
          <a class="tag" href="https://rese1f.github.io/aurora-web/">Website</a>
          <a class="tag" href="https://huggingface.co/collections/wchai/auroracap-66d117ffe13bedda96702013">Model</a>
          <a class="tag" href="https://huggingface.co/datasets/wchai/Video-Detailed-Caption">Benchmark</a>
          <!-- <a class="tag" href="https://huggingface.co/datasets/wchai/AuroraCap-trainset">Dataset</a> -->
          <a class="tag" href="https://github.com/rese1f/aurora">Code</a>
          <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/aurora?style=social">
        </div>
        <span >AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning.</span>
        <!-- <b><span >Conference on Neural Information Processing Systems (NeurIPS 2024)</span></b> -->
        <!-- <b><span style="color: red;"> Best Paper Award @ NeurIPS 2023 DLDE</span></b> -->
      </div>
    </div>

    <!-- <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/Fantasy.png">
      </div>
      <div class="pub-right">
        <div class="title">
          Fantasy: Transformer Meets Transformer in Text-to-Image Generation
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*</a>,
            <a class="author" href="">Xun Guo</a>,
            <a class="author" href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>,
            <a class="author" href="">Yan Lu</a>
          </font>
        </div>
        <div class="publish">
          <span class="publisher">Preperint, 2024</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://openreview.net/forum?id=qL4nN6Ew7U">Paper</a>
        </div>
        <span >Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM). </span>
      </div>
    </div> -->

    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/moviechat.gif">
      </div>
      <div class="pub-right">
        <div class="title">
          MovieChat: From Dense Token to Sparse Memory for Long Video Understanding
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*</a>,
            <a class="author" href="">Guanhong Wang*</a>,
            <a class="author" href="">Yucheng Zhang</a>,
            <a class="author" href="">Haoyang Zhou</a>,
            <a class="author" href="">Feiyang Wu</a>,
            <a class="author" href="">Xun Guo</a>,
            <a class="author" href="">Tian Ye</a>,
            <a class="author" href="">Yan Lu</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>,
            <a class="author" href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>

          </font>
        </div>
        <div class="publish">
          <span class="publisher">CVPR, 2024</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          <a class="tag" href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html">Paper</a>
          <a class="tag" href="https://rese1f.github.io/MovieChat/">Website</a>
          <a class="tag" href="https://huggingface.co/datasets/Enxin/MovieChat-1K_train">Benchmark</a>
          <a class="tag" href="https://github.com/rese1f/MovieChat">Code</a>
          <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
        </div>
        <span >MovieChat achieves state-of-the-art performace in extra long video (more than 10K frames) understanding by introducing memory mechanism. </span>
      </div>
    </div>

    
  </div>
</div>
</div>


<div class="wrapper">
  <div id="academicservices" class="bio">
    <div class="bio-body" style="overflow:hidden;">
      <h1 class="md-heading text-left">
        <i class="fa fa-tasks" aria-hidden="true"></i>
        Professional Service
      </h1>  
      <li>
        <strong>Conference and Journal Refereeing:</strong> <br>
        &nbsp&nbsp&nbsp&nbsp NeurIPS 2025, PRCV 2023&2025, CVPR 2025, ICLR 2025&2026, TMM 2024, TPAMI 2025<br>
      </li>
      <li>
        <strong>Workshop Organization:</strong> <br>
        &nbsp&nbsp&nbsp&nbsp Multimodal Video Agent Workshop on CVPR 2025<br>
        &nbsp&nbsp&nbsp&nbsp Long-form Video Understanding Towards Multimodal AI Assistant and Copilot Workshop on CVPR 2024<br>
      </li>
    </div>
  </div>
</div>



<div class="wrapper">
  <div id="teaching" class="bio">
      <h1 class="md-heading text-left">
        <i class="fa fa-tasks" aria-hidden="true"></i>
        Teaching Assistant
      </h1>  
      <li>
        <strong>ECE 445 Senior Design (Undergraduate) - Sping 2024</strong><br>
        &nbsp&nbsp&nbsp&nbsp Teaching Assistant (TA), with <a href="https://person.zju.edu.cn/en/gaoangwang/">Prof. Gaoang Wang</a>
      </li>
  </div>
</div>

<div class="wrapper">
  <div id="awards" class="bio">
    <h1 class="md-heading text-left">
      <i class="fa fa-tasks" aria-hidden="true"></i>
      Selected Honors & Awards
    </h1>
    <ul>
      <li>  KAUST Rising Stars in AI Symposium 2026</li>
      <li><a href="https://lambda.ai/?matchtype=p&adgroup=167368709642&feeditemid=&loc_interest_ms=&loc_physical_ms=9053131&network=g&device=c&devicemodel=&adposition=&utm_source=google&utm_campaign=Google_Search_Cloud-Brand&utm_medium=search&utm_term=lambda%20ai&utm_content=708570750692&hsa_acc=1731978716&hsa_cam=17699749392&hsa_grp=167368709642&hsa_ad=708570750692&hsa_src=g&hsa_tgt=kwd-554422468680&hsa_kw=lambda%20ai&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gad_source=1&gad_campaignid=17699749392&gbraid=0AAAAADrJiRzfUWNIrFKK1cSqVBfmCYcwO&gclid=Cj0KCQjwl5jHBhDHARIsAB0YqjwyVXl0iQvzsjkmAdZ7O4xSkSCjV5rVoJmwDkoGMi-z2I31lZAIPbwaAtIPEALw_wcB" target="_blank" rel="noopener">Lambda AI</a> Cloud Credits Grant Sponsorship, 2025</li>
      <li> National Scholarship, 2025 (Zhejiang University)</li>
      <li> National Scholarship, 2024 (Zhejiang University)</li>
      <li> National Scholarship, 2021 (Dalian University of Technology)</li>
    </ul>
  </div>
</div>



<footer class="site-footer">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Personal Email:</li>
          <li><a href="mailto:">ensong [at] ucsd.edu</a></li>
          <li><a href="mailto:">enxin.23 [at] intl.zju.edu.cn</a></li>
          <li><a href="mailto:">enxin.song.dut [at] gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
          <a href="https://scholar.google.com.hk/citations?user=sLqa-3oAAAAJ&hl=zh-CN" class="icon-button github">
          <i class="ai ai-google-scholar-square icon-github"></i>
            <span></span>
          </a>

          <a href="https://github.com/Espere-1119-Song" class="icon-button github">
            <i class="fa fa-github icon-github"></i>
            <span></span>
          </a>

          <a href="https://linkedin.com/in/enxinsong-57b471286/" class="icon-button linkedin">
            <i class="fa fa-linkedin icon-linkedin"></i>
            <span></span>
          </a>
          
          <a href="https://twitter.com/EnxinSong" class="icon-button twitter">
            <i class="fa fa-twitter icon-twitter"></i>
            <span></span>
          </a>
         
          <a href="mailto:enxin.23@intl.zju.edu.cn" class="icon-button github">
            <i class="fa fa-envelope icon-github"></i>
            <span></span>
          </a>




      </div>
	<div class="footer-col footer-col-3">
    <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fespere-1119-song.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
    <p class="text">Last updated: Nov, 2024.<br>The style of this website is borrowed from <a href='https://www.hexianghu.com/'>Hexiang Hu's</a>.</p>
	</div>
  </div>
  </div>

</footer>
    <div class="back-to-top">Top</div>

<script type="text/javascript">
jQuery(document).ready(function() {
    var offset = 220;
    var duration = 500;
    jQuery(window).scroll(function() {
        if (jQuery(this).scrollTop() > offset) {
            jQuery('.back-to-top').fadeIn(duration);
        } else {
            jQuery('.back-to-top').fadeOut(duration);
    
        }
    });
    jQuery('.back-to-top').click(function(event) {
        event.preventDefault();
        jQuery('html, body').animate({scrollTop: 0}, duration);
        return false;
    })
});
</script>
  </body>

<!--
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60071442-1', 'auto');
  ga('send', 'pageview');
</script>
-->

<!--<script src="custom-pub.js"></script>-->
</html>
